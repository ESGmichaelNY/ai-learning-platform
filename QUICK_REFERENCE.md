# AI Model Comparison Lab - Quick Reference

## What You Got

A fully functional Next.js web app with:
- âœ… Interactive UI to compare AI models
- âœ… Side-by-side output display
- âœ… Cost/speed metrics with rankings
- âœ… Save & load experiments
- âœ… 3 example prompts to start
- âœ… Mock data (zero cost)
- âœ… Backend proxy pattern (keys stay secure)
- âœ… Supabase integration ready
- âœ… TypeScript + Tailwind CSS

---

## Get It Running (5 minutes)

### Step 1: Install
```bash
cd ai-learning-platform
npm install
```

### Step 2: Set Up Supabase (optional for MVP)
- Go to https://supabase.com, create free account
- Create a project
- Copy `.env.local.example` â†’ `.env.local`
- Paste your Supabase URL and key into `.env.local`
- Create the two tables (SQL in SETUP.md)

### Step 3: Run
```bash
npm run dev
```
Open http://localhost:3000

---

## How It Works

**User Flow:**
1. Enter prompt (or use example)
2. Select models to compare (Claude, GPT, Gemini, Llama)
3. Click "Compare Models"
4. See results side-by-side with cost/speed rankings
5. Save experiment (stored locally for now)

**Behind the Scenes:**
- Frontend (React) â†’ Backend API route â†’ Mock data (for now)
- No API keys needed yet (mock data only)
- When you're ready, swap mock data for real API calls

---

## File Map

| File | Purpose |
|------|---------|
| `app/page.tsx` | Main UI (all the UI code) |
| `app/api/compare/route.ts` | Comparison engine |
| `lib/mockData.ts` | Mock responses + example prompts |
| `lib/supabase.ts` | Database connection |
| `.env.local` | Your secrets (API keys) |
| `SETUP.md` | Detailed setup guide |
| `README.md` | Full documentation |

---

## Key Features to Understand

### Mock Data Strategy
Currently using pre-written responses. This means:
- âœ… UI works perfectly without real APIs
- âœ… Zero cost during development
- âœ… Predictable for testing
- â†ªï¸ When ready: Replace `getMockResponse()` with real API calls

### Backend Proxy
API keys never touch the browser:
- Frontend calls `/api/compare` (public)
- Backend handles authentication (private)
- Results returned to frontend
- Users never see your keys âœ“

### Experiment Saving
Current state: Saves to browser memory (resets on refresh)
Next step: Uncomment Supabase calls in UI to persist to database

---

## Next Steps (When Ready)

### Add Real APIs (30 min)
1. Get API keys (Claude, OpenAI, Google)
2. Install SDKs: `npm install @anthropic-ai/sdk openai @google/generative-ai`
3. Update `app/api/compare/route.ts` to use real calls instead of mock data
4. Add keys to `.env.local`
5. Done!

### Activate Database Saving (15 min)
1. Create Supabase tables (SQL in SETUP.md)
2. Uncomment the save/load functions in UI
3. Test saving and loading experiments

### Deploy (5 min)
```bash
vercel deploy
```
Or use any Node.js hosting (Railway, Render, etc.)

---

## Example: How Mock Data Works

File: `lib/mockData.ts`
```typescript
export const MOCK_RESPONSES = {
  "Explain photosynthesis to a 10-year-old": {
    "claude-3-sonnet": {
      text: "Photosynthesis is how plants make their own food...",
      tokens: 87,
      speed: 0.045,
      cost: 0.015
    },
    "gpt-4": { ... },
    // etc
  }
}
```

When user clicks compare, the backend:
```typescript
const response = getMockResponse(prompt, modelId)
// Returns the pre-written text + metrics
```

To switch to real API:
```typescript
const response = await anthropic.messages.create({...})
// Actually calls the API
```

---

## Faculty Learning Workflow

**Session 1:** "Same prompt, different models"
- Use example prompts or try their own
- Compare outputs side-by-side
- Notice differences in tone, length, accuracy
- Discuss: Which model is "best"? For what?

**Session 2:** (Coming) "Prompt matters"
- Slightly change the prompt
- See how outputs change
- Learn: Good prompts help all models

**Session 3:** (Coming) "Speed & cost trade-offs"
- Understand when to use each model
- Fast models vs. smart models
- Budget implications for large-scale use

---

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Port 3000 in use | `npm run dev -- -p 3001` |
| Missing dependencies | `npm install` |
| Blank screen | Check browser console for errors |
| No mock data | Verify `lib/mockData.ts` exists |
| Supabase errors | Check URL/key in `.env.local` |

---

## File Checklist

You should have these files:
```
ai-learning-platform/
â”œâ”€â”€ app/page.tsx âœ“
â”œâ”€â”€ app/api/compare/route.ts âœ“
â”œâ”€â”€ app/layout.tsx âœ“
â”œâ”€â”€ app/globals.css âœ“
â”œâ”€â”€ lib/supabase.ts âœ“
â”œâ”€â”€ lib/mockData.ts âœ“
â”œâ”€â”€ package.json âœ“
â”œâ”€â”€ next.config.js âœ“
â”œâ”€â”€ tailwind.config.js âœ“
â”œâ”€â”€ tsconfig.json âœ“
â”œâ”€â”€ postcss.config.js âœ“
â”œâ”€â”€ .env.local.example âœ“
â”œâ”€â”€ .gitignore âœ“
â”œâ”€â”€ README.md âœ“
â”œâ”€â”€ SETUP.md âœ“
â””â”€â”€ This file (QUICK_REFERENCE.md) âœ“
```

---

## Ready to Go!

1. Run `npm install`
2. Run `npm run dev`
3. Open http://localhost:3000
4. Try the example prompts
5. Explore the UI

Questions? Check SETUP.md or README.md for details.

Good luck! ğŸš€
